ggplot(aes(x = accommodates)) +
geom_point(aes(y = price)) +
geom_line(aes(y = pred), color = 'red')
#' Now, what if we wanted to *quantify* how well the model predicts
#' these out-of-sample values? There are several metrics to aggregate
#' the prediction error. We'll look at the coefficient of determination:
summary(lm1)
R2
OSR2
R2 <- 1 - sum((pred_train - listingsTrain$price) ^ 2) /
sum((mean(listingsTrain$price) - listingsTrain$price) ^ 2)
pred_test <- predict(lm1, newdata = listingsTest)
OSR2 <- 1 - sum((pred_test - listingsTest$price) ^ 2) /
sum((mean(listingsTrain$price) - listingsTest$price) ^ 2)
pred_train <- predict(lm1)
R2 <- 1 - sum((pred_train - listingsTrain$price) ^ 2) /
sum((mean(listingsTrain$price) - listingsTrain$price) ^ 2)
pred_test <- predict(lm1, newdata = listingsTest)
OSR2 <- 1 - sum((pred_test - listingsTest$price) ^ 2) /
sum((mean(listingsTrain$price) - listingsTest$price) ^ 2)
summary(lm1)
R2
OSR2
listings %>% select(amenities) %>% head()
source("clean_amenities.R")
listings_clean <- clean_amenities(listings)
listingsBig <- listings_clean %>%
filter(!is.na(review_scores_rating),
accommodates <= 10,
property_type %in% c("Apartment", "House", "Bed & Breakfast",
"Condominium", "Loft", "Townhouse"),
!(neighbourhood_cleansed %in%
c("Leather District","Longwood Medical Area")),
price <= 1000) %>%
select(price, accommodates, room_type, property_type,
review_scores_rating, neighbourhood_cleansed,
starts_with("amenity"))
set.seed(123)
spl <- sample.split(listingsBig$price, SplitRatio = 0.7)
listingsBigTrain <- subset(listingsBig, spl == TRUE)
listingsBigTest <- subset(listingsBig, spl == FALSE)
#' To get R to learn the model, we need to pass it a formula.
#' We don't want to write down all those amenity variables by hand.
#' Luckily, we can use the `paste()` function to string all the
#' variable names together, and then the `as.formula()` function
#' to translate a string into a formula.
amenities_string <- listingsBigTrain %>%
select(starts_with("amenity")) %>%
names() %>%
paste(collapse = " + ")
amenities_string # Taking a look to make sure things worked
big_formula <- as.formula(paste("price ~ accommodates + accommodates*room_type + property_type + neighbourhood_cleansed + property_type*neighbourhood_cleansed + review_scores_rating*neighbourhood_cleansed + accommodates*review_scores_rating", amenities_string, sep = " + "))
big_formula
#' Now we can use the `lm()` function:
lm2 <- lm(big_formula, data = listingsBigTrain)
#' What happens when we compare in-sample and
#' out-of-sample prediction performance?
pred_test <- predict(lm2, newdata = listingsBigTest)
OSR2_2 <- 1 - sum((pred_test - listingsBigTest$price) ^ 2) /
sum((mean(listingsBigTrain$price) - listingsBigTest$price) ^ 2)
summary(lm2)$r.squared # In-sample
OSR2_2 # Out-of-sample
OSR2
summary(lm2)$r.squared # In-sample
OSR2_2 # Out-of-sample
OSR2_2 # Out-of-sample
OSR2 # Original model
OSR2_2 # Out-of-sample
summary(lm2)$r.squared # In-sample
OSR2_2 # Out-of-sample
OSR2 # Original model
library(glmnet)
#' Notice that `glmnet()` doesn't communicate with the data via
#' formulas. Instead, it wants a matrix of predictor variables and
#' a vector of values for the variable we're trying to predict,
x <- model.matrix(~ . - price + accommodates*room_type +
property_type*neighbourhood_cleansed +
review_scores_rating*neighbourhood_cleansed +
accommodates*review_scores_rating,
data = listingsBig)
y <- as.vector(listingsBig$price)
xTrain <- x[spl, ]
xTest <- x[!spl, ]
yTrain <- y[spl]
yTest <- y[!spl]
lasso_price <- glmnet(xTrain, yTrain)
#' This time the `summary()` function isn't quite as useful:
summary(lasso_price)
#' It does give us some info, though. Notice that "lambda" is a
#' vector of length 88. The `glmnet()` function has defined 88
#' different values of lambda and found the corresponding optimal
#' beta vector for each one! We have 88 different models here.
#' Let's look at some of the coefficients for the different models.
#' We'll start with one where lambda is really high:
lasso_price$lambda[1]
nnzero(lasso_price$beta[, 1]) # How many coefficients are nonzero?
#' Here the penalty on the size of the coefficients is so high
#' that R sets them all to zero. Moving to some smaller lambdas:
lasso_price$lambda[10]
lasso_price$beta[which(lasso_price$beta[, 10] != 0), 10]
lasso_price$lambda[20]
lasso_price$beta[which(lasso_price$beta[, 20] != 0), 20]
#' And, to see the whole path of lambdas:
plot.glmnet(lasso_price, xvar = "lambda")
#' Here, each line is one variable. The plot is quite messy with
#' so many variables, but it gives us the idea. As lambda shrinks,
#' the model adds more and more nonzero coefficients.
#' ### Cross-Validation
#' How do we choose which of the 88 models to use? Or in other words,
#' how do we "tune" the $\lambda$ parameter? We'll use a similar idea
#' to the training-test set split called cross-validation.
#' The idea behind cross-validation is this: what if we trained our
#' family of models (in this case 88) on only some of the training data
#' and left out some other data points? Then we could use those other
#' data points to figure out which of the lambdas works best
#' out-of-sample. So we'd have a training set for training all the
#' models, a validation set for choosing the best one, and a test set
#' to evaluate performance once we've settled on a model.
#' There's just one other trick: since taking more samples
#' reduces noise, could we somehow take more validation set
#' samples? Here's where *cross*-validation comes in.
#' We divide the training data into groups called *folds*,
#' and for each fold repeat the train-validate procedure on
#' the remaining training data and use the current fold as a
#' validation set. We then average the performance of each model
#' on each fold and pick the best one.
#' This is a very common *resampling* method that applies in lots and
#' lots of settings. Lucky for us the glmnet package has a very handy
#' function called `cv.glmnet()` which does the entire process
#' automatically. Let's look at the function arguments using `?cv.glmnet`.
#' The relevant arguments for us right now are
#' * x, the matrix of predictors
#' * y, the response variable
#' * nfolds, the number of ways to split the training set (defaults to 10)
#' * type.measure, the metric of prediction quality. It defaults to
#' mean squared error, the square of RMSE, for linear regression
#' Let's do the cross-validation:
lasso_price_cv = cv.glmnet(xTrain, yTrain)
summary(lasso_price_cv) # What does the model object look like?
#' Notice the "lambda.min". This is the best lambda as determined by
#' the cross validation. "lambda.1se" is the largest lambda such that
#' the "error is within 1 standard error of the minimum."
#' There's another automatic plotting function for `cv.glmnet()`
#' which shows the error for each model:
plot.cv.glmnet(lasso_price_cv)
#' The first vertical dotted line shows `lambda.min`, and the
#' second is `lambda.1se`. The figure illustrates that we cross-validate
#' to find the "sweet spot" where there's not too much bias (high lambda)
#' and not too much noise (low lambda). The left-hand side of this graph
#' is flatter than we'd sometimes see, meaning that the unpenalized model
#' may not be too bad. However, increasing lambda increases
#' interpretability at close to no loss in prediction accuracy!
#' Let's again compare training and test error. Because we are using
#' glmnet we need to use the specialized `predict.cv.glmnet()` function:
pred_train <- predict.cv.glmnet(lasso_price_cv, newx = xTrain)
R2_lasso <- 1 - sum((pred_train - yTrain) ^ 2) /
sum((mean(yTrain) - yTrain) ^ 2)
pred_test <- predict.cv.glmnet(lasso_price_cv, newx = xTest)
OSR2_lasso <- 1 - sum((pred_test - yTest) ^ 2) /
sum((mean(yTrain) - yTest) ^ 2)
R2_lasso
OSR2_lasso
OSR2
OSR2_2 # Overfit model
OSR2_lasso
x <- model.matrix(big_formula,
data = listingsBig)
y <- as.vector(listingsBig$price)
xTrain <- x[spl, ]
xTest <- x[!spl, ]
yTrain <- y[spl]
yTest <- y[!spl]
lasso_price <- glmnet(xTrain, yTrain)
#' This time the `summary()` function isn't quite as useful:
summary(lasso_price)
#' It does give us some info, though. Notice that "lambda" is a
#' vector of length 88. The `glmnet()` function has defined 88
#' different values of lambda and found the corresponding optimal
#' beta vector for each one! We have 88 different models here.
#' Let's look at some of the coefficients for the different models.
#' We'll start with one where lambda is really high:
lasso_price$lambda[1]
nnzero(lasso_price$beta[, 1]) # How many coefficients are nonzero?
#' Here the penalty on the size of the coefficients is so high
#' that R sets them all to zero. Moving to some smaller lambdas:
lasso_price$lambda[10]
lasso_price$beta[which(lasso_price$beta[, 10] != 0), 10]
lasso_price$lambda[20]
lasso_price$beta[which(lasso_price$beta[, 20] != 0), 20]
#' And, to see the whole path of lambdas:
plot.glmnet(lasso_price, xvar = "lambda")
#' Here, each line is one variable. The plot is quite messy with
#' so many variables, but it gives us the idea. As lambda shrinks,
#' the model adds more and more nonzero coefficients.
#' ### Cross-Validation
#' How do we choose which of the 88 models to use? Or in other words,
#' how do we "tune" the $\lambda$ parameter? We'll use a similar idea
#' to the training-test set split called cross-validation.
#' The idea behind cross-validation is this: what if we trained our
#' family of models (in this case 88) on only some of the training data
#' and left out some other data points? Then we could use those other
#' data points to figure out which of the lambdas works best
#' out-of-sample. So we'd have a training set for training all the
#' models, a validation set for choosing the best one, and a test set
#' to evaluate performance once we've settled on a model.
#' There's just one other trick: since taking more samples
#' reduces noise, could we somehow take more validation set
#' samples? Here's where *cross*-validation comes in.
#' We divide the training data into groups called *folds*,
#' and for each fold repeat the train-validate procedure on
#' the remaining training data and use the current fold as a
#' validation set. We then average the performance of each model
#' on each fold and pick the best one.
#' This is a very common *resampling* method that applies in lots and
#' lots of settings. Lucky for us the glmnet package has a very handy
#' function called `cv.glmnet()` which does the entire process
#' automatically. Let's look at the function arguments using `?cv.glmnet`.
#' The relevant arguments for us right now are
#' * x, the matrix of predictors
#' * y, the response variable
#' * nfolds, the number of ways to split the training set (defaults to 10)
#' * type.measure, the metric of prediction quality. It defaults to
#' mean squared error, the square of RMSE, for linear regression
#' Let's do the cross-validation:
lasso_price_cv = cv.glmnet(xTrain, yTrain)
summary(lasso_price_cv) # What does the model object look like?
#' Notice the "lambda.min". This is the best lambda as determined by
#' the cross validation. "lambda.1se" is the largest lambda such that
#' the "error is within 1 standard error of the minimum."
#' There's another automatic plotting function for `cv.glmnet()`
#' which shows the error for each model:
plot.cv.glmnet(lasso_price_cv)
#' The first vertical dotted line shows `lambda.min`, and the
#' second is `lambda.1se`. The figure illustrates that we cross-validate
#' to find the "sweet spot" where there's not too much bias (high lambda)
#' and not too much noise (low lambda). The left-hand side of this graph
#' is flatter than we'd sometimes see, meaning that the unpenalized model
#' may not be too bad. However, increasing lambda increases
#' interpretability at close to no loss in prediction accuracy!
#' Let's again compare training and test error. Because we are using
#' glmnet we need to use the specialized `predict.cv.glmnet()` function:
pred_train <- predict.cv.glmnet(lasso_price_cv, newx = xTrain)
R2_lasso <- 1 - sum((pred_train - yTrain) ^ 2) /
sum((mean(yTrain) - yTrain) ^ 2)
pred_test <- predict.cv.glmnet(lasso_price_cv, newx = xTest)
OSR2_lasso <- 1 - sum((pred_test - yTest) ^ 2) /
sum((mean(yTrain) - yTest) ^ 2)
R2_lasso
OSR2_lasso
OSR2 # Original Model
OSR2_2 # Overfit model
#' The overfitting problem has gotten better, but hasn't yet gone
OSR2_2 # Overfit model
plot.glmnet(lasso_price, xvar = "lambda")
big_formula
big_formula
paste(big_formula)
paste(big_formula)[3]
big_formula[1]
big_formula
delete.response(big_formula)
big_formula
model_formula <- delete.response(big_formula)
model_formula
model_formula <- drop.terms(big_formula)
model_formula <- drop.terms(big_formula, keep.response = FALSE)
model_formula
paste(model_formula)
model_formula <- reformulate(big_formula)
drop.terms(big_formula, keep.response = FALSE)
terms(big_formula)
delete.response(terms(big_formula))
reformulate(delete.response(terms(big_formula)))
as.formula(delete.response(terms(big_formula)))
model_formula <- as.formula(delete.response(terms(big_formula)))
model_formula
model_formula <- delete.response(terms(big_formula))
model_formula <- delete.response(terms(big_formula))$term.labels
model_formula
model_formula <- delete.response(terms(big_formula))["term.labels"]
model_formula
model_formula <- delete.response(terms(big_formula))
model_formula
str(model_formula)
update(big_formula, - price ~ .)
update(big_formula, ~ .)
model_formula <- update(big_formula, ~ .)
model_formula
tt <- terms(big_formula)
tt
delete.response(tt)
reformulate(attr(tt, "term.labels"))
big_formula
tt <- terms(big_formula)
delete.response(tt)
reformulate(attr(tt, "term.labels"))
tt <- terms(big_formula)
delete.response(tt)
model_formula <- reformulate(attr(tt, "term.labels"))
model_formula <- update(big_formula, ~ .)
model_formula <- reformulate(attr(tt, "term.labels"))
tt <- delete.response(terms(big_formula))
model_formula <- reformulate(attr(tt, "term.labels"))
model_formula
model_formula <- big_formula
model_formula
model_formula <- big_formula %>%
terms() %>%
delete.response()
model_formula
model_formula <- big_formula %>%
terms() %>%
delete.response() %>%
attr("term.labels")
model_formula <- big_formula %>%
terms() %>%
delete.response() %>%
attr("term.labels") %>%
reformulate()
model_formula
model_formula <- big_formula %>%
terms() %>%
delete.response() %>%
attr("term.labels") %>%
reformulate()
model_formula
x <- model.matrix(model_formula,
data = listingsBig)
y <- as.vector(listingsBig$price)
xTrain <- x[spl, ]
xTest <- x[!spl, ]
yTrain <- y[spl]
yTest <- y[!spl]
lasso_price <- glmnet(xTrain, yTrain)
#' This time the `summary()` function isn't quite as useful:
summary(lasso_price)
#' It does give us some info, though. Notice that "lambda" is a
#' vector of length 88. The `glmnet()` function has defined 88
#' different values of lambda and found the corresponding optimal
#' beta vector for each one! We have 88 different models here.
#' Let's look at some of the coefficients for the different models.
#' We'll start with one where lambda is really high:
lasso_price$lambda[1]
nnzero(lasso_price$beta[, 1]) # How many coefficients are nonzero?
#' Here the penalty on the size of the coefficients is so high
#' that R sets them all to zero. Moving to some smaller lambdas:
lasso_price$lambda[10]
lasso_price$beta[which(lasso_price$beta[, 10] != 0), 10]
lasso_price$lambda[20]
lasso_price$beta[which(lasso_price$beta[, 20] != 0), 20]
#' And, to see the whole path of lambdas:
plot.glmnet(lasso_price, xvar = "lambda")
#' Here, each line is one variable. The plot is quite messy with
#' so many variables, but it gives us the idea. As lambda shrinks,
#' the model adds more and more nonzero coefficients.
#' ### Cross-Validation
#' How do we choose which of the 88 models to use? Or in other words,
#' how do we "tune" the $\lambda$ parameter? We'll use a similar idea
#' to the training-test set split called cross-validation.
#' The idea behind cross-validation is this: what if we trained our
#' family of models (in this case 88) on only some of the training data
#' and left out some other data points? Then we could use those other
#' data points to figure out which of the lambdas works best
#' out-of-sample. So we'd have a training set for training all the
#' models, a validation set for choosing the best one, and a test set
#' to evaluate performance once we've settled on a model.
#' There's just one other trick: since taking more samples
#' reduces noise, could we somehow take more validation set
#' samples? Here's where *cross*-validation comes in.
#' We divide the training data into groups called *folds*,
#' and for each fold repeat the train-validate procedure on
#' the remaining training data and use the current fold as a
#' validation set. We then average the performance of each model
#' on each fold and pick the best one.
#' This is a very common *resampling* method that applies in lots and
#' lots of settings. Lucky for us the glmnet package has a very handy
#' function called `cv.glmnet()` which does the entire process
#' automatically. Let's look at the function arguments using `?cv.glmnet`.
#' The relevant arguments for us right now are
#' * x, the matrix of predictors
#' * y, the response variable
#' * nfolds, the number of ways to split the training set (defaults to 10)
#' * type.measure, the metric of prediction quality. It defaults to
#' mean squared error, the square of RMSE, for linear regression
#' Let's do the cross-validation:
lasso_price_cv = cv.glmnet(xTrain, yTrain)
summary(lasso_price_cv) # What does the model object look like?
#' Notice the "lambda.min". This is the best lambda as determined by
#' the cross validation. "lambda.1se" is the largest lambda such that
#' the "error is within 1 standard error of the minimum."
#' There's another automatic plotting function for `cv.glmnet()`
#' which shows the error for each model:
plot.cv.glmnet(lasso_price_cv)
#' The first vertical dotted line shows `lambda.min`, and the
#' second is `lambda.1se`. The figure illustrates that we cross-validate
#' to find the "sweet spot" where there's not too much bias (high lambda)
#' and not too much noise (low lambda). The left-hand side of this graph
#' is flatter than we'd sometimes see, meaning that the unpenalized model
#' may not be too bad. However, increasing lambda increases
#' interpretability at close to no loss in prediction accuracy!
#' Let's again compare training and test error. Because we are using
#' glmnet we need to use the specialized `predict.cv.glmnet()` function:
pred_train <- predict.cv.glmnet(lasso_price_cv, newx = xTrain)
R2_lasso <- 1 - sum((pred_train - yTrain) ^ 2) /
sum((mean(yTrain) - yTrain) ^ 2)
pred_test <- predict.cv.glmnet(lasso_price_cv, newx = xTest)
OSR2_lasso <- 1 - sum((pred_test - yTest) ^ 2) /
sum((mean(yTrain) - yTest) ^ 2)
R2_lasso
OSR2_lasso
OSR2 # Original Model
OSR2_2 # Overfit model
model_formula <- big_formula %>%
paste()
big_formula
?gsub
model_formula <- gsub("price", "", paste(big_formula))
model_formula
model_formula <- reformulate(gsub("price", "", paste(big_formula)))
model_formula
model_formula <- as.formula(gsub("price", "", paste(big_formula)))
model_formula
x <- model.matrix(model_formula,
data = listingsBig)
y <- as.vector(listingsBig$price)
xTrain <- x[spl, ]
xTest <- x[!spl, ]
yTrain <- y[spl]
yTest <- y[!spl]
lasso_price <- glmnet(xTrain, yTrain)
#' This time the `summary()` function isn't quite as useful:
summary(lasso_price)
#' It does give us some info, though. Notice that "lambda" is a
#' vector of length 88. The `glmnet()` function has defined 88
#' different values of lambda and found the corresponding optimal
#' beta vector for each one! We have 88 different models here.
#' Let's look at some of the coefficients for the different models.
#' We'll start with one where lambda is really high:
lasso_price$lambda[1]
nnzero(lasso_price$beta[, 1]) # How many coefficients are nonzero?
#' Here the penalty on the size of the coefficients is so high
#' that R sets them all to zero. Moving to some smaller lambdas:
lasso_price$lambda[10]
lasso_price$beta[which(lasso_price$beta[, 10] != 0), 10]
lasso_price$lambda[20]
lasso_price$beta[which(lasso_price$beta[, 20] != 0), 20]
#' And, to see the whole path of lambdas:
plot.glmnet(lasso_price, xvar = "lambda")
#' Here, each line is one variable. The plot is quite messy with
#' so many variables, but it gives us the idea. As lambda shrinks,
#' the model adds more and more nonzero coefficients.
#' ### Cross-Validation
#' How do we choose which of the 88 models to use? Or in other words,
#' how do we "tune" the $\lambda$ parameter? We'll use a similar idea
#' to the training-test set split called cross-validation.
#' The idea behind cross-validation is this: what if we trained our
#' family of models (in this case 88) on only some of the training data
#' and left out some other data points? Then we could use those other
#' data points to figure out which of the lambdas works best
#' out-of-sample. So we'd have a training set for training all the
#' models, a validation set for choosing the best one, and a test set
#' to evaluate performance once we've settled on a model.
#' There's just one other trick: since taking more samples
#' reduces noise, could we somehow take more validation set
#' samples? Here's where *cross*-validation comes in.
#' We divide the training data into groups called *folds*,
#' and for each fold repeat the train-validate procedure on
#' the remaining training data and use the current fold as a
#' validation set. We then average the performance of each model
#' on each fold and pick the best one.
#' This is a very common *resampling* method that applies in lots and
#' lots of settings. Lucky for us the glmnet package has a very handy
#' function called `cv.glmnet()` which does the entire process
#' automatically. Let's look at the function arguments using `?cv.glmnet`.
#' The relevant arguments for us right now are
#' * x, the matrix of predictors
#' * y, the response variable
#' * nfolds, the number of ways to split the training set (defaults to 10)
#' * type.measure, the metric of prediction quality. It defaults to
#' mean squared error, the square of RMSE, for linear regression
#' Let's do the cross-validation:
lasso_price_cv = cv.glmnet(xTrain, yTrain)
summary(lasso_price_cv) # What does the model object look like?
#' Notice the "lambda.min". This is the best lambda as determined by
#' the cross validation. "lambda.1se" is the largest lambda such that
#' the "error is within 1 standard error of the minimum."
#' There's another automatic plotting function for `cv.glmnet()`
#' which shows the error for each model:
plot.cv.glmnet(lasso_price_cv)
#' The first vertical dotted line shows `lambda.min`, and the
#' second is `lambda.1se`. The figure illustrates that we cross-validate
#' to find the "sweet spot" where there's not too much bias (high lambda)
#' and not too much noise (low lambda). The left-hand side of this graph
#' is flatter than we'd sometimes see, meaning that the unpenalized model
#' may not be too bad. However, increasing lambda increases
#' interpretability at close to no loss in prediction accuracy!
#' Let's again compare training and test error. Because we are using
#' glmnet we need to use the specialized `predict.cv.glmnet()` function:
pred_train <- predict.cv.glmnet(lasso_price_cv, newx = xTrain)
R2_lasso <- 1 - sum((pred_train - yTrain) ^ 2) /
sum((mean(yTrain) - yTrain) ^ 2)
pred_test <- predict.cv.glmnet(lasso_price_cv, newx = xTest)
OSR2_lasso <- 1 - sum((pred_test - yTest) ^ 2) /
sum((mean(yTrain) - yTest) ^ 2)
R2_lasso
OSR2_lasso
OSR2 # Original Model
OSR2_2 # Overfit model
lasso_price$lambda[20]
lasso_price$beta[which(lasso_price$beta[, 20] != 0), 20]
#' And, to see the whole path of lambdas:
plot.glmnet(lasso_price, xvar = "lambda")
